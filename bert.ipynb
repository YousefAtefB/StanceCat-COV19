{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Dataset/train.csv')\n",
    "text_train, category_train, stance_train = train['text'], train['category'], train['stance']\n",
    "\n",
    "dev = pd.read_csv('Dataset/dev.csv')\n",
    "text_dev, category_dev, stance_dev = dev['text'], dev['category'], dev['stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
       "      <td>personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
       "      <td>personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   category  stance\n",
       "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...  celebrity       1\n",
       "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...  info_news       1\n",
       "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...  info_news       1\n",
       "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...  celebrity       1\n",
       "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...   personal       0\n",
       "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...  info_news       0\n",
       "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...  info_news       1\n",
       "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...   personal       0\n",
       "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...  unrelated       0\n",
       "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...  info_news       1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#مريم_رجوي: &lt;LF&gt;حظر خامنئي المجرم شراء #لقاح_ك...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#الصحة:&lt;LF&gt;•تم إعطاء 259.530 جرعة من لقاح #كور...</td>\n",
       "      <td>plan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#خادم_الحرمين - حفظه الله - يتلقى الجرعة الأول...</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#الصحه_العالميه: لقاحات #كورونا آمنة ولا خوف م...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#وزيرة_الصحة \"#هالة_زايد\" تقول إنه يجرى مراجعة...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2️⃣ وانتهى الفريق من الدراسات قبل السريرية ونش...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>عاجل 🔴 &lt;LF&gt;.&lt;LF&gt;&lt;LF&gt;.&lt;LF&gt;&lt;LF&gt;وزارة الصحة :&lt;LF&gt;...</td>\n",
       "      <td>plan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#فيديو | السفير الأميركي لدى السعودية بعد تلقي...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>تصريحات وبس الحكومة مع السيسي علي حسب اللقطة! ...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>الاتحاد الاوروبي تفاوض لشراء لقاحات الكورونا م...</td>\n",
       "      <td>info_news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   category  stance\n",
       "0  #مريم_رجوي: <LF>حظر خامنئي المجرم شراء #لقاح_ك...  info_news       1\n",
       "1  #الصحة:<LF>•تم إعطاء 259.530 جرعة من لقاح #كور...       plan       1\n",
       "2  #خادم_الحرمين - حفظه الله - يتلقى الجرعة الأول...  celebrity       1\n",
       "3  #الصحه_العالميه: لقاحات #كورونا آمنة ولا خوف م...  info_news       1\n",
       "4  #وزيرة_الصحة \"#هالة_زايد\" تقول إنه يجرى مراجعة...  info_news       1\n",
       "5  2️⃣ وانتهى الفريق من الدراسات قبل السريرية ونش...  info_news       1\n",
       "6  عاجل 🔴 <LF>.<LF><LF>.<LF><LF>وزارة الصحة :<LF>...       plan       1\n",
       "7  #فيديو | السفير الأميركي لدى السعودية بعد تلقي...  info_news       1\n",
       "8  تصريحات وبس الحكومة مع السيسي علي حسب اللقطة! ...  info_news       0\n",
       "9  الاتحاد الاوروبي تفاوض لشراء لقاحات الكورونا م...  info_news       1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6988,) (6988,) (6988,)\n",
      "(1000,) (1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "text_train, category_train, stance_train = np.array(train['text']), np.array(train['category']), np.array(train['stance'])\n",
    "text_dev, category_dev, stance_dev = np.array(dev['text']), np.array(dev['category']), np.array(dev['stance'])\n",
    "\n",
    "print(text_train.shape, category_train.shape, stance_train.shape)\n",
    "print(text_dev.shape, category_dev.shape, stance_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessing(text):\n",
    "\n",
    "    # remove links\n",
    "    text = [re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) for x in text]\n",
    "    # text = [re.sub(r'https?:\\/\\/\\S*', '', x, flags=re.MULTILINE) for x in text]\n",
    "\n",
    "    # remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = [emoji_pattern.sub(r'', x) for x in text] # no emoji\n",
    "\n",
    "    # remove english words\n",
    "    text = [re.sub(r'\\s*[A-Za-z]+\\b', '' , x) for x in text]\n",
    "\n",
    "    # tokenize\n",
    "    text = [nltk.tokenize.word_tokenize(x) for x in text]\n",
    "\n",
    "    # # remove stop-words\n",
    "    # stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
    "\n",
    "    # for i in range(len(text)):\n",
    "    #     text[i] = [word for word in text[i] if word not in stopwords]\n",
    "\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        text[i] = [word for word in text[i] if len(word)>2]\n",
    "\n",
    "    # but anything in empty strings\n",
    "    for i in range(len(text)):\n",
    "        if(len(text[i])==0):\n",
    "            text[i]='<unk>'\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name=\"aubmindlab/bert-base-arabertv02-twitter\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "\n",
    "text = \"ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري\"\n",
    "arabert_prep.preprocess(text)\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02-twitter\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"aubmindlab/bert-base-arabertv02-twitter\")\n",
    "\n",
    "def PreProcessing2(text):\n",
    "    # print(text[10])\n",
    "\n",
    "    text = [arabert_prep.preprocess(x) for x in text]\n",
    "    # print(text[10])\n",
    "    \n",
    "    text = tokenizer(text)\n",
    "    # print(len(text[\"input_ids\"][10]),text[\"input_ids\"][10])\n",
    "\n",
    "    text = torch.nn.utils.rnn.pad_sequence([torch.tensor(sentence) for sentence in text[\"input_ids\"]], batch_first=True, padding_value=0)\n",
    "    # print(text[10])\n",
    "\n",
    "    text = model(text).logits\n",
    "    # print(text[10].shape)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PreProcessing2(text_train[:512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text_train[2]\n",
    "# print(text)\n",
    "# text = arabert_prep.preprocess(text)\n",
    "# print(text)\n",
    "# text = tokenizer(text)\n",
    "# print(print(text),torch.tensor(text['input_ids']).unsqueeze(0).shape)\n",
    "# text = model(torch.tensor(text['input_ids']).unsqueeze(0))\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الامريكيين متهمون بصنع ونشر فيروس كورونا ولذلك لا يمكن الوثوق بهم”<LF>الإمام الخامنئي<LF><LF>#لقاح_آمن\tinfo_news\t0\n",
      "train\tحبيبنا وقرة أعيننا سيدي #خادم_الحرمين_الشريفين الملك سلمان حفظه الله يتلقى الجرعة الأولى من لقاح كورنا … نفعه الله به ومتعه بالصحة والعافيه. https://t.co/AJRzC7dCWe\tcelebrity\t1\n",
      "train\tرغم تلقيه جرعتين من لقاح #فايزر.. إصابة كبير حاخامات #تل_أبيب، يسرائيل لاو،  83 عاما، بفيروس #كورونا، حيث انتقلت له العدوى من زوجته بعد مخالطتها مصابا آخر https://t.co/RGI6WTgrxf\tcelebrity\t0\n",
      "train\tتلقيت قبل قليل الجرعة الثانية من لقاح كورونا، وكلي فخر بجهود وطننا الغالي وتوجيهات قيادتنا الرشيدة التي تؤكد أن صحة الإنسان أولاً.  🇸🇦🇸🇦🇸🇦🇸🇦 https://t.co/XGstr9Zvzf\tinfo_news\t1\n",
      "train\tشركة صحة\": جزيل الشكر للمواطنة ملهية شويرب سعيد العامري، التي تبلغ ١٠٢ عاماً<LF>لكونها قدوة لجميع أفراد المجتمع من خلال <LF> تلقيها أول جرعة من لقاح كوفيد-19 في مركز القوع الصحي #الإمارات_اليوم https://t.co/uBSCd0JZ4Y\n"
     ]
    }
   ],
   "source": [
    "print(max(text_train, key=len))\n",
    "text_train = PreProcessing(text_train)\n",
    "text_dev = PreProcessing(text_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['بيل', 'غيتس', 'يتلقى', 'لقاح', 'كوفيد19', 'غير', 'تصوير', 'الابرة', 'السيرنجة', 'الدواء', 'لابس', 'بولو', 'صيفي', 'الشتاء', 'يقول', 'إحدى', 'مزايا', 'عمر', 'عامًا', 'انه', 'مؤهل', 'للحصول', 'على', 'اللقاح', '...', 'يعنى', 'كان', 'يحتاج', 'اللقاح', 'كان', 'عمره', 'اصغر']\n"
     ]
    }
   ],
   "source": [
    "with open('processed_train.txt','w', encoding='utf8') as f:\n",
    "\tfor i in text_train:\n",
    "\t\tf.write('%s\\n'%i)\n",
    "print(text_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildVocab(text, pad='<pad>', unk='<unk>'):\n",
    "\n",
    "    vocab = set()    \n",
    "    for x in text:\n",
    "        vocab |= set(x)\n",
    "\n",
    "    vocab = [pad, unk] + list(vocab)\n",
    "\n",
    "    id2word = {i: word for i, word in enumerate(vocab)}\n",
    "    word2id = {word: i for i, word in id2word.items()}\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    return vocab_size, vocab, id2word, word2id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32058\n"
     ]
    }
   ],
   "source": [
    "vocab_size, vocab, id2word, word2id = BuildVocab(text_train)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'others', 'personal', 'restrictions', 'info_news', 'rumors', 'unrelated', 'celebrity', 'advice', 'plan', 'requests'}\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "categories = set(category_train)\n",
    "print(categories)\n",
    "category2id = {word:i for i, word in enumerate(list(categories))}\n",
    "print(category2id['celebrity'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to try\n",
    "1) bi-directional\n",
    "2) pre-training\n",
    "3) multi-layers\n",
    "4) BERT\n",
    "5) transformers notebook\n",
    "6) packed_padded_sequences\n",
    "7) pre-trained embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, pad='<pad>', unk='<unk>', word2id=word2id):\n",
    "\n",
    "    x = x.copy()\n",
    "\n",
    "    # src lengths to be used in pack padded\n",
    "    self.seq_lengths = torch.LongTensor(list(map(len, x)))\n",
    "\n",
    "    print(x[0], self.seq_lengths[0])\n",
    "\n",
    "    for i in range(len(x)):\n",
    "      x[i] = [word2id[word] if word in word2id else word2id[unk] for word in x[i]]\n",
    "    \n",
    "    print(x[0])\n",
    "\n",
    "    self.X = torch.nn.utils.rnn.pad_sequence([torch.tensor(sentence) for sentence in x], batch_first=True, padding_value=word2id[pad])\n",
    "\n",
    "    # sort sequeces decreasing in size\n",
    "    self.seq_lengths, perm_idx = self.seq_lengths.sort(0, descending=True)\n",
    "    self.X = self.X[perm_idx]\n",
    "\n",
    "    print(self.X[0])\n",
    "\n",
    "    print(self.X.shape)\n",
    "\n",
    "    print(min(self.seq_lengths))\n",
    "    \n",
    "    self.Y = torch.tensor(y)\n",
    "    self.len = len(x)\n",
    "    self.pad = pad\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.Y[idx], self.seq_lengths[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['بيل', 'غيتس', 'يتلقى', 'لقاح', 'كوفيد19', 'غير', 'تصوير', 'الابرة', 'السيرنجة', 'الدواء', 'لابس', 'بولو', 'صيفي', 'الشتاء', 'يقول', 'إحدى', 'مزايا', 'عمر', 'عامًا', 'انه', 'مؤهل', 'للحصول', 'على', 'اللقاح', '...', 'يعنى', 'كان', 'يحتاج', 'اللقاح', 'كان', 'عمره', 'اصغر'] tensor(32)\n",
      "[19009, 24724, 1963, 12351, 17173, 11074, 28364, 11439, 28735, 3431, 30322, 20742, 11516, 18377, 24930, 20171, 7087, 12166, 16670, 27485, 27990, 30191, 19956, 28836, 16585, 21888, 30814, 15008, 28836, 30814, 1942, 31652]\n",
      "tensor([27908, 23445, 22255, 12523, 16578, 13183, 18058,  7816, 14310, 23064,\n",
      "        12393, 18856, 23126, 13955, 25158, 27186,  1427, 19281, 14591, 29717,\n",
      "        19131, 30914, 22503,  1963, 10699, 25253, 12351, 26524, 14492, 22503,\n",
      "        29403, 18659, 29603,  1341, 29876, 23100, 12351,   366, 27996,  9990,\n",
      "         2718, 29625, 23961, 20627, 11784, 13069,  9861, 14021,  5904, 31739,\n",
      "        15402,  1334,  6912, 24401, 15860, 20931, 22684, 10761, 10699,  1149,\n",
      "        12351,  9861, 23599, 31136, 17604, 30620, 17271, 28635,  8250, 24387,\n",
      "        25242, 18638, 22046, 20128,  6882, 14170, 22046, 21152,  8927,  1211,\n",
      "        30455, 12564, 16217, 14513, 25242,  5622,  1013, 28624,  2840,   278,\n",
      "        17343,   606, 23721, 31840, 30565,  1480, 25900, 12351,  6159,  5266,\n",
      "        11720, 30007, 24516])\n",
      "torch.Size([6988, 103])\n",
      "tensor(1)\n",
      "['بيل', 'غيتس', 'يتلقى', 'لقاح', 'كوفيد19', 'غير', 'تصوير', 'الابرة', 'السيرنجة', 'الدواء', 'لابس', 'بولو', 'صيفي', 'الشتاء', 'يقول', 'إحدى', 'مزايا', 'عمر', 'عامًا', 'انه', 'مؤهل', 'للحصول', 'على', 'اللقاح', '...', 'يعنى', 'كان', 'يحتاج', 'اللقاح', 'كان', 'عمره', 'اصغر'] tensor(32)\n",
      "[19009, 24724, 1963, 12351, 17173, 11074, 28364, 11439, 28735, 3431, 30322, 20742, 11516, 18377, 24930, 20171, 7087, 12166, 16670, 27485, 27990, 30191, 19956, 28836, 16585, 21888, 30814, 15008, 28836, 30814, 1942, 31652]\n",
      "tensor([27908, 23445, 22255, 12523, 16578, 13183, 18058,  7816, 14310, 23064,\n",
      "        12393, 18856, 23126, 13955, 25158, 27186,  1427, 19281, 14591, 29717,\n",
      "        19131, 30914, 22503,  1963, 10699, 25253, 12351, 26524, 14492, 22503,\n",
      "        29403, 18659, 29603,  1341, 29876, 23100, 12351,   366, 27996,  9990,\n",
      "         2718, 29625, 23961, 20627, 11784, 13069,  9861, 14021,  5904, 31739,\n",
      "        15402,  1334,  6912, 24401, 15860, 20931, 22684, 10761, 10699,  1149,\n",
      "        12351,  9861, 23599, 31136, 17604, 30620, 17271, 28635,  8250, 24387,\n",
      "        25242, 18638, 22046, 20128,  6882, 14170, 22046, 21152,  8927,  1211,\n",
      "        30455, 12564, 16217, 14513, 25242,  5622,  1013, 28624,  2840,   278,\n",
      "        17343,   606, 23721, 31840, 30565,  1480, 25900, 12351,  6159,  5266,\n",
      "        11720, 30007, 24516])\n",
      "torch.Size([6988, 103])\n",
      "tensor(1)\n",
      "['مريم_رجوي', 'حظر', 'خامنئي', 'المجرم', 'شراء', 'لقاح_كورونا', 'يعد', 'مجزرة', 'متعمدة', 'بحق', 'الشعب', 'الإيراني', 'نقل', 'موقع', 'مريم', 'رجوي', 'موقف', 'رئيسة', 'الجمهورية', 'المنتخبة', 'للمقاومة', 'الإيرانية', 'تصريحات', 'خامنئي', 'المجرم', 'حول', 'حظر', 'استيراد', 'لقاح', 'كورونا', 'الولايات', 'المتحدة', 'بريطانيا', 'فرنسا', 'اللقاح_حق_للناس'] tensor(35)\n",
      "[19996, 5099, 11358, 14590, 290, 9638, 12253, 11991, 5183, 12239, 8918, 17815, 26982, 5191, 7504, 1257, 6647, 4199, 18508, 1, 1, 13474, 11912, 11358, 14590, 11692, 5099, 12864, 12351, 13183, 18179, 18735, 17533, 16915, 20243]\n",
      "tensor([ 7814,     1, 26182, 28836,     1,     1, 23825, 25953,     1, 27118,\n",
      "         4587,  4051, 10529, 14170, 24196, 14864,     1, 12351,     1,     1,\n",
      "        12099, 25190,     1,     1, 21422, 31322, 11263, 23058, 23300, 19229,\n",
      "        16260,     1, 11665,     1, 11852, 31281,  5744, 28561,  4749, 17076,\n",
      "            1, 11852,  6263,     1,  3192])\n",
      "torch.Size([1000, 45])\n",
      "tensor(2)\n",
      "['مريم_رجوي', 'حظر', 'خامنئي', 'المجرم', 'شراء', 'لقاح_كورونا', 'يعد', 'مجزرة', 'متعمدة', 'بحق', 'الشعب', 'الإيراني', 'نقل', 'موقع', 'مريم', 'رجوي', 'موقف', 'رئيسة', 'الجمهورية', 'المنتخبة', 'للمقاومة', 'الإيرانية', 'تصريحات', 'خامنئي', 'المجرم', 'حول', 'حظر', 'استيراد', 'لقاح', 'كورونا', 'الولايات', 'المتحدة', 'بريطانيا', 'فرنسا', 'اللقاح_حق_للناس'] tensor(35)\n",
      "[19996, 5099, 11358, 14590, 290, 9638, 12253, 11991, 5183, 12239, 8918, 17815, 26982, 5191, 7504, 1257, 6647, 4199, 18508, 1, 1, 13474, 11912, 11358, 14590, 11692, 5099, 12864, 12351, 13183, 18179, 18735, 17533, 16915, 20243]\n",
      "tensor([ 7814,     1, 26182, 28836,     1,     1, 23825, 25953,     1, 27118,\n",
      "         4587,  4051, 10529, 14170, 24196, 14864,     1, 12351,     1,     1,\n",
      "        12099, 25190,     1,     1, 21422, 31322, 11263, 23058, 23300, 19229,\n",
      "        16260,     1, 11665,     1, 11852, 31281,  5744, 28561,  4749, 17076,\n",
      "            1, 11852,  6263,     1,  3192])\n",
      "torch.Size([1000, 45])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "stance_train_dataset = Dataset(text_train, stance_train + 1)\n",
    "category_train_dataset = Dataset(text_train, [category2id[category] for category in category_train])\n",
    "\n",
    "stance_dev_dataset = Dataset(text_dev, stance_dev + 1)\n",
    "category_dev_dataset = Dataset(text_dev, [category2id[category] for category in category_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "\n",
    "        packed_embedded =  torch.nn.utils.rnn.pack_padded_sequence(embedded, src_len.cpu().numpy(), batch_first=False)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        #outputs = [src len, batch size, hid dim]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        #outputs are always from the top hidden layer\n",
    "\n",
    "        prediction = self.fc_out(hidden)\n",
    "        #prediction = [1, batch size, output dim]\n",
    "\n",
    "        prediction = prediction.squeeze(0)\n",
    "        #prediction = [batch size, output dim]\n",
    "\n",
    "        return prediction        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, train_dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    total_acc_train = 0.0\n",
    "    total_loss_train = 0.0\n",
    "\n",
    "    for train_input, train_label, src_len in tqdm(train_dataloader):\n",
    "\n",
    "        train_input = train_input.to(device).permute(1, 0)\n",
    "        # print(train_input.shape)\n",
    "        train_label = train_label.to(device)\n",
    "\n",
    "        output = model(train_input, src_len)\n",
    "        # print(output.shape, train_label.shape)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.view(-1, output_dim)\n",
    "        train_label = train_label.view(-1)\n",
    "\n",
    "        batch_loss = criterion(output, train_label)\n",
    "\n",
    "        total_loss_train += batch_loss\n",
    "        \n",
    "        acc = torch.sum(torch.argmax(output, -1) == train_label) \n",
    "        total_acc_train += acc\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    # calculate loss    \n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    epoch_acc = total_acc_train / len(train_dataset)\n",
    "\n",
    "    # calculate f1 score\n",
    "    train_input, train_label, src_len = train_dataset[:]\n",
    "    train_input = train_input.to(device).permute(1, 0)\n",
    "    train_label = train_label.to(device)\n",
    "    output = model(train_input, src_len)\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output.view(-1, output_dim)\n",
    "    train_label = train_label.view(-1)\n",
    "    y_true, y_pred = train_label, torch.argmax(output, -1)\n",
    "\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return epoch_loss, epoch_acc, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, test_dataloader, criterion):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  total_acc_test = 0.0\n",
    "  total_loss_test = 0.0\n",
    "  \n",
    "  with torch.no_grad():\n",
    "\n",
    "    for test_input, test_label, src_len in tqdm(test_dataloader):\n",
    "\n",
    "      test_input = test_input.to(device).permute(1, 0)\n",
    "      test_label = test_label.to(device)\n",
    "\n",
    "\n",
    "      output = model(test_input, src_len)\n",
    "\n",
    "      batch_loss = criterion(output.view(-1, model.output_dim), test_label.view(-1))\n",
    "\n",
    "      total_loss_test += batch_loss\n",
    "\n",
    "      acc = torch.sum(torch.argmax(output, -1)==test_label)\n",
    "      total_acc_test += acc\n",
    "\n",
    "    # calculate loss\n",
    "    total_loss_test /= len(test_dataset)\n",
    "\n",
    "    # calculate accuracy\n",
    "    total_acc_test /= len(test_dataset)\n",
    "\n",
    "    # calculate f1 score\n",
    "    test_input, test_label, src_len = test_dataset[:]\n",
    "    test_input = test_input.to(device).permute(1, 0)\n",
    "    test_label = test_label.to(device)\n",
    "    output = model(test_input, src_len)\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output.view(-1, output_dim)\n",
    "    test_label = test_label.view(-1)\n",
    "    y_true, y_pred = test_label, torch.argmax(output, -1)\n",
    "\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "  \n",
    "  return total_loss_test, total_acc_test, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, train_dataset, dev_dataset, model_name, batch_size=512, epochs=10, learning_rate=0.01, clip=1):\n",
    "\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "  dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "  # criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  \n",
    "  model = model.to(device)\n",
    "  criterion = criterion.to(device)\n",
    "\n",
    "  best_f1_macro = 0\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "\n",
    "    epoch_loss, epoch_acc, train_f1_macro = train(model, train_dataset, train_dataloader, optimizer, criterion, clip)\n",
    "    dev_loss, dev_acc, dev_f1_macro = evaluate(model, dev_dataset, dev_dataloader, criterion)\n",
    "\n",
    "    if dev_f1_macro > best_f1_macro:\n",
    "      best_f1_macro = dev_f1_macro\n",
    "      torch.save(model.state_dict(), 'best_'+model_name+'.pt')\n",
    "\n",
    "    print(f'Train = Epochs: {epoch_num + 1} | Loss: {epoch_loss} | Accuracy: {epoch_acc} | f1_macro : {train_f1_macro}')\n",
    "    print(f'Dev = Epochs: {epoch_num + 1} | Loss: {dev_loss} | Accuracy: {dev_acc} | f1_macro : {dev_f1_macro}')    \n",
    "\n",
    "  model.load_state_dict(torch.load('best_'+model_name+'.pt'))\n",
    "\n",
    "  dev_loss, dev_acc, dev_f1_macro = evaluate(model, dev_dataset, dev_dataloader, criterion)\n",
    "\n",
    "  print(f'Best Dev = Loss: {dev_loss} | Accuracy: {dev_acc} | f1_macro : {dev_f1_macro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (4236606597.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [22], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    HID_DIM = 50gi #512\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = vocab_size\n",
    "OUTPUT_DIM = 3\n",
    "EMB_DIM = 50 #256\n",
    "HID_DIM = 50gi #512\n",
    "DROPOUT = 0.5\n",
    "\n",
    "stance_model = LSTM(INPUT_DIM, EMB_DIM, HID_DIM, OUTPUT_DIM, DROPOUT).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "stance_model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(stance_model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate(stance_model, stance_train_dataset, stance_dev_dataset, 'stance_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = vocab_size\n",
    "OUTPUT_DIM = 10\n",
    "EMB_DIM = 50 #256\n",
    "HID_DIM = 50 #512\n",
    "DROPOUT = 0.5\n",
    "\n",
    "category_model = LSTM(INPUT_DIM, EMB_DIM, HID_DIM, OUTPUT_DIM, DROPOUT).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "category_model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(category_model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate(category_model, category_train_dataset, category_dev_dataset, 'category_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(stance_model, stance_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(category_model, category_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size=512, epochs=10, learning_rate=0.01):\n",
    "  \n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "  # criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  \n",
    "  model = model.to(device)\n",
    "  criterion = criterion.to(device)\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0.0\n",
    "    total_loss_train = 0.0\n",
    "\n",
    "    for train_input, train_label, src_len in tqdm(train_dataloader):\n",
    "\n",
    "      train_input = train_input.to(device).permute(1, 0)\n",
    "      # print(train_input.shape)\n",
    "      train_label = train_label.to(device)\n",
    "\n",
    "      output = model(train_input, src_len)\n",
    "\n",
    "      # print(output.shape, train_label.shape)\n",
    "      \n",
    "      batch_loss = criterion(output.view(-1, model.output_dim), train_label.view(-1))\n",
    "\n",
    "      total_loss_train += batch_loss\n",
    "      \n",
    "      acc = torch.sum(torch.argmax(output, -1) == train_label) \n",
    "      total_acc_train += acc\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      batch_loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      \n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    epoch_acc = total_acc_train / len(train_dataset)\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, batch_size=512):\n",
    "\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  model = model.to(device)\n",
    "\n",
    "  total_acc_test = 0.0\n",
    "  \n",
    "  with torch.no_grad():\n",
    "\n",
    "    for test_input, test_label, src_len in tqdm(test_dataloader):\n",
    "\n",
    "      test_input = test_input.to(device).permute(1, 0)\n",
    "      test_label = test_label.to(device)\n",
    "\n",
    "\n",
    "      output = model(test_input, src_len)\n",
    "\n",
    "      acc = torch.sum(torch.argmax(output, -1)==test_label)\n",
    "      total_acc_test += acc\n",
    "    \n",
    "    total_acc_test /= len(test_dataset)\n",
    "  \n",
    "  print(f'\\nDev Accuracy: {total_acc_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f707ed687e1cc7dca614d866740125e744cc3f7963ec2d63a60d682146be2e45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
