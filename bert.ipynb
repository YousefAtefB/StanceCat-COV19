{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YousefAtefB/StanceCat-COV19/blob/main/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK7AjxdBjW1Z"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/StanceCat-COV19"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWG1FbUr7TRn",
        "outputId": "6b4e99c1-5e91-4202-b8b5-523d44f488c7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/StanceCat-COV19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Cg_iBd0RjW1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd07e80-b261-4ba9-e631-97f5825324aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import re\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "nltk.download('punkt')\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfTzru_-jW1e"
      },
      "source": [
        "# Data PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WrTbA7l-jW1f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('Dataset/train.csv')\n",
        "text_train, category_train, stance_train = train['text'], train['category'], train['stance']\n",
        "\n",
        "dev = pd.read_csv('Dataset/dev.csv')\n",
        "text_dev, category_dev, stance_dev = dev['text'], dev['category'], dev['stance']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GgkZM03DjW1g",
        "outputId": "89b896ba-0acb-4b95-abbc-c8691666e220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text   category  stance\n",
              "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...  celebrity       1\n",
              "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...  info_news       1\n",
              "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...  info_news       1\n",
              "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...  celebrity       1\n",
              "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...   personal       0\n",
              "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...  info_news       0\n",
              "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...  info_news       1\n",
              "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...   personal       0\n",
              "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...  unrelated       0\n",
              "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...  info_news       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6f7afa0e-cc88-42eb-9585-d2cfe0b8891d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f7afa0e-cc88-42eb-9585-d2cfe0b8891d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6f7afa0e-cc88-42eb-9585-d2cfe0b8891d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6f7afa0e-cc88-42eb-9585-d2cfe0b8891d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "train.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9V9-M9z4jW1g",
        "outputId": "14006c52-e2a3-4825-fe3a-f96f89c4ee6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text   category  stance\n",
              "0  #مريم_رجوي: <LF>حظر خامنئي المجرم شراء #لقاح_ك...  info_news       1\n",
              "1  #الصحة:<LF>•تم إعطاء 259.530 جرعة من لقاح #كور...       plan       1\n",
              "2  #خادم_الحرمين - حفظه الله - يتلقى الجرعة الأول...  celebrity       1\n",
              "3  #الصحه_العالميه: لقاحات #كورونا آمنة ولا خوف م...  info_news       1\n",
              "4  #وزيرة_الصحة \"#هالة_زايد\" تقول إنه يجرى مراجعة...  info_news       1\n",
              "5  2️⃣ وانتهى الفريق من الدراسات قبل السريرية ونش...  info_news       1\n",
              "6  عاجل 🔴 <LF>.<LF><LF>.<LF><LF>وزارة الصحة :<LF>...       plan       1\n",
              "7  #فيديو | السفير الأميركي لدى السعودية بعد تلقي...  info_news       1\n",
              "8  تصريحات وبس الحكومة مع السيسي علي حسب اللقطة! ...  info_news       0\n",
              "9  الاتحاد الاوروبي تفاوض لشراء لقاحات الكورونا م...  info_news       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6372c511-e421-4203-bac6-290ed07419a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#مريم_رجوي: &lt;LF&gt;حظر خامنئي المجرم شراء #لقاح_ك...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#الصحة:&lt;LF&gt;•تم إعطاء 259.530 جرعة من لقاح #كور...</td>\n",
              "      <td>plan</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#خادم_الحرمين - حفظه الله - يتلقى الجرعة الأول...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#الصحه_العالميه: لقاحات #كورونا آمنة ولا خوف م...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#وزيرة_الصحة \"#هالة_زايد\" تقول إنه يجرى مراجعة...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2️⃣ وانتهى الفريق من الدراسات قبل السريرية ونش...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>عاجل 🔴 &lt;LF&gt;.&lt;LF&gt;&lt;LF&gt;.&lt;LF&gt;&lt;LF&gt;وزارة الصحة :&lt;LF&gt;...</td>\n",
              "      <td>plan</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>#فيديو | السفير الأميركي لدى السعودية بعد تلقي...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>تصريحات وبس الحكومة مع السيسي علي حسب اللقطة! ...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>الاتحاد الاوروبي تفاوض لشراء لقاحات الكورونا م...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6372c511-e421-4203-bac6-290ed07419a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6372c511-e421-4203-bac6-290ed07419a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6372c511-e421-4203-bac6-290ed07419a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "dev.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5ZY3ifXHjW1h",
        "outputId": "2df50b91-25ad-41f1-d1f8-601bcc4fe555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6988,) (6988,) (6988,)\n",
            "(1000,) (1000,) (1000,)\n"
          ]
        }
      ],
      "source": [
        "text_train, category_train, stance_train = np.array(train['text']), np.array(train['category']), np.array(train['stance'])\n",
        "text_dev, category_dev, stance_dev = np.array(dev['text']), np.array(dev['category']), np.array(dev['stance'])\n",
        "\n",
        "print(text_train.shape, category_train.shape, stance_train.shape)\n",
        "print(text_dev.shape, category_dev.shape, stance_dev.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arabert\n",
        "!pip install transformers\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "model_name=\"aubmindlab/bert-base-arabertv02-twitter\"\n",
        "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "\n",
        "text = \"ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري\"\n",
        "arabert_prep.preprocess(text)\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02-twitter\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"aubmindlab/bert-base-arabertv02-twitter\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtntXFJCSzCe",
        "outputId": "74c69bdd-e6b1-448b-d1ae-a4c2e2bb8e7d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arabert in /usr/local/lib/python3.8/dist-packages (1.0.1)\n",
            "Requirement already satisfied: PyArabic in /usr/local/lib/python3.8/dist-packages (from arabert) (0.6.15)\n",
            "Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.8/dist-packages (from arabert) (1.4.2)\n",
            "Requirement already satisfied: farasapy in /usr/local/lib/python3.8/dist-packages (from arabert) (0.0.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farasapy->arabert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farasapy->arabert) (4.64.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from PyArabic->arabert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = text_train[0]\n",
        "bert_input = tokenizer(example_text, padding='max_length', max_length = 100, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "print(bert_input['input_ids'])\n",
        "print(bert_input['token_type_ids'])\n",
        "print(bert_input['attention_mask'])\n",
        "\n",
        "output = model(**bert_input)\n",
        "print(output.logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rMErblT8ZUI-",
        "outputId": "55ec2f98-a29c-4673-afbf-268cafe75fa1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    2,  6941, 28652, 16136, 34495,    10, 17448,   196, 13593,   306,\n",
            "           650,  5166, 48342,   197,   139,   391,  5787,  7325,   197,   139,\n",
            "           391, 11474,   139,  7829,   213,  3616,   185, 54749,   305,  2858,\n",
            "         10639,   139,  1721,   338,  2779, 17613,  1686,   298,  5263,     1,\n",
            "           634,   889, 13591,  5175,   323, 28892,    20,    20,    20,  9110,\n",
            "           394,   418,  3682, 28892,  1422,   418,  6843, 36720,   306,  5263,\n",
            "         62279,    77,   221,   221,   178,   223,    31,     1,     1,    89,\n",
            "            20,    72,   219,     1,    54,   275,   272,   244,   244,   270,\n",
            "           260,   264,   268,   179,     3,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])\n",
            "torch.Size([1, 100, 64000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YajWdDqojW1k",
        "outputId": "24d3a635-9500-4e14-a04c-d3261cb92b5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'personal', 'advice', 'requests', 'unrelated', 'restrictions', 'others', 'info_news', 'plan', 'celebrity', 'rumors'}\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "categories = set(category_train)\n",
        "print(categories)\n",
        "category2id = {word:i for i, word in enumerate(list(categories))}\n",
        "print(category2id['celebrity'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNLJr0azjW1l"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVlYMy5KjW1l"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnzUMpPajW1l"
      },
      "source": [
        "### Ideas to try\n",
        "1) bi-directional\n",
        "2) pre-training\n",
        "3) multi-layers\n",
        "4) BERT\n",
        "5) transformers notebook\n",
        "6) packed_padded_sequences\n",
        "7) pre-trained embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4d1daadjW1l"
      },
      "source": [
        "### Building Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "v_PIOF-0jW1l"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, x, y):\n",
        "\n",
        "    x = x.copy()\n",
        "\n",
        "    x = [arabert_prep.preprocess(text) for text in x]\n",
        "    self.X = [tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for text in x]\n",
        "\n",
        "    self.Y = torch.tensor(y)\n",
        "\n",
        "    self.len = len(x)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.Y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "j41ekReTjW1m"
      },
      "outputs": [],
      "source": [
        "stance_train_dataset = Dataset(text_train, stance_train + 1)\n",
        "category_train_dataset = Dataset(text_train, [category2id[category] for category in category_train])\n",
        "\n",
        "stance_dev_dataset = Dataset(text_dev, stance_dev + 1)\n",
        "category_dev_dataset = Dataset(text_dev, [category2id[category] for category in category_dev])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KNWkSYiTjW1m"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('aubmindlab/bert-base-arabertv02-twitter')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Kny-X5tgjW1m"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4mx_qdjW1m"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataset, train_dataloader, criterion, optimizer):\n",
        "\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "\n",
        "  # for f1 score\n",
        "  y_true, y_pred = [], []\n",
        "\n",
        "  for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "    # print(\"input_ids\",train_input['input_ids'].shape)\n",
        "    # print(\"attention_mask\",train_input['attention_mask'].shape)\n",
        "\n",
        "    train_label = train_label.to(device)\n",
        "    mask = train_input['attention_mask'].to(device)\n",
        "    input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "    output = model(input_id, mask)\n",
        "\n",
        "    # print(output.shape)\n",
        "\n",
        "    batch_loss = criterion(output, train_label.long())\n",
        "    total_loss_train += batch_loss.item()\n",
        "\n",
        "    acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "    total_acc_train += acc\n",
        "\n",
        "    y_true += train_label.tolist() \n",
        "    y_pred += output.argmax(dim=1).tolist()\n",
        "\n",
        "    model.zero_grad()\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # print(len(y_true), len(y_pred))\n",
        "  f1_macro_train = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "  return total_loss_train/len(train_dataset), total_acc_train/len(train_dataset), f1_macro_train"
      ],
      "metadata": {
        "id": "_551g6SEfIfx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_dataset, test_dataloader, criterion):\n",
        "\n",
        "  total_acc_test = 0\n",
        "  total_loss_test = 0\n",
        "\n",
        "  # for f1 score\n",
        "  y_true, y_pred = [], []\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for test_input, test_label in test_dataloader:\n",
        "\n",
        "      test_label = test_label.to(device)\n",
        "\n",
        "      # print(test_input['input_ids'].shape)\n",
        "      # print(\"attention_mask\",train_input['attention_mask'].shape)\n",
        "\n",
        "      mask = test_input['attention_mask'].to(device)\n",
        "      input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "      output = model(input_id, mask)\n",
        "\n",
        "      # print(output.shape)\n",
        "\n",
        "      batch_loss = criterion(output, test_label.long())\n",
        "      total_loss_test += batch_loss.item()\n",
        "\n",
        "      acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "      total_acc_test += acc\n",
        "\n",
        "      y_true += test_label.tolist() \n",
        "      y_pred += output.argmax(dim=1).tolist()\n",
        "\n",
        "  # print(len(y_true)-1, len(y_pred)-1)\n",
        "  f1_macro_test = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "  return total_loss_test/len(test_dataset), total_acc_test/len(test_dataset), f1_macro_test"
      ],
      "metadata": {
        "id": "_Zg2G9hNDxh8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate(model, train_dataset, val_dataset, learning_rate, epochs, model_name):\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2)\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "  best_f1_macro = 0\n",
        "\n",
        "  for epoch_num in range(epochs):\n",
        "\n",
        "    loss_train, acc_train, f1_macro_train = train(model, train_dataset, train_dataloader, criterion, optimizer)\n",
        "    loss_val, acc_val, f1_macro_val = evaluate(model, val_dataset, val_dataloader, criterion)\n",
        "          \n",
        "    if f1_macro_val > best_f1_macro:\n",
        "      best_f1_macro = f1_macro_val\n",
        "    torch.save(model.state_dict(), f'Models/BERT/{model_name}/F1{f1_macro_val: .4f} Acc{acc_val: .4f}.pt')\n",
        "\n",
        "    print(\n",
        "        f'Epochs: {epoch_num + 1} \\\n",
        "        | Train Loss: {loss_train: .4f} \\\n",
        "        | Train Accuracy: {acc_train : .4f} \\\n",
        "        | Train F1_macro: {f1_macro_train: .4f} \\\n",
        "        | Val Loss: {loss_val: .4f} \\\n",
        "        | Val Accuracy: {acc_val: .4f} \\\n",
        "        | Val F1_macro: {f1_macro_val: .4f} \\\n",
        "        ')"
      ],
      "metadata": {
        "id": "umKC5RVvDzkE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "OUTPUT_DIM = 3\n",
        "LR = 1e-6\n",
        "\n",
        "model = BertClassifier(OUTPUT_DIM)\n",
        "              \n",
        "train_evaluate(model, stance_train_dataset, stance_dev_dataset, LR, EPOCHS, 'Stance')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_Se-eux3fiMd",
        "outputId": "ce38377b-aaf2-4d2d-e6b7-34bc17b599e0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02-twitter were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 1/1 [00:21<00:00, 21.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1         | Train Loss:  0.558         | Train Accuracy:  0.000         | Train F1_macro:  0.000         | Val Loss:  0.501         | Val Accuracy:  0.500         | Val F1_macro:  0.333         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:17<00:00, 17.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2         | Train Loss:  0.549         | Train Accuracy:  0.000         | Train F1_macro:  0.000         | Val Loss:  0.540         | Val Accuracy:  0.500         | Val F1_macro:  0.333         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:13<00:00, 13.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3         | Train Loss:  0.486         | Train Accuracy:  0.500         | Train F1_macro:  0.333         | Val Loss:  0.607         | Val Accuracy:  0.000         | Val F1_macro:  0.000         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:13<00:00, 13.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4         | Train Loss:  0.555         | Train Accuracy:  0.000         | Train F1_macro:  0.000         | Val Loss:  0.563         | Val Accuracy:  0.500         | Val F1_macro:  0.333         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:14<00:00, 14.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5         | Train Loss:  0.578         | Train Accuracy:  0.000         | Train F1_macro:  0.000         | Val Loss:  0.549         | Val Accuracy:  0.000         | Val F1_macro:  0.000         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "OUTPUT_DIM = 10\n",
        "LR = 1e-6\n",
        "\n",
        "model = BertClassifier(OUTPUT_DIM)\n",
        "              \n",
        "train_evaluate(model, category_train_dataset, category_dev_dataset, LR, EPOCHS, 'Category')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsZiircpVUpn",
        "outputId": "52e120ee-8e90-4df1-aa2f-234b8096d42d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02-twitter were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 1/1 [00:27<00:00, 27.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1         | Train Loss:  1.228         | Train Accuracy:  0.000         | Train F1_macro:  0.000         | Val Loss:  1.141         | Val Accuracy:  0.000         | Val F1_macro:  0.000         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:29<00:00, 29.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2         | Train Loss:  1.092         | Train Accuracy:  0.000         | Train F1_macro:  0.000         | Val Loss:  1.181         | Val Accuracy:  0.000         | Val F1_macro:  0.000         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:13<00:00, 13.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3         | Train Loss:  1.174         | Train Accuracy:  0.000         | Train F1_macro:  0.000         | Val Loss:  1.251         | Val Accuracy:  0.000         | Val F1_macro:  0.000         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:18<00:00, 18.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4         | Train Loss:  0.957         | Train Accuracy:  1.000         | Train F1_macro:  1.000         | Val Loss:  1.239         | Val Accuracy:  0.000         | Val F1_macro:  0.000         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:23<00:00, 23.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5         | Train Loss:  1.052         | Train Accuracy:  0.000         | Train F1_macro:  0.000         | Val Loss:  1.196         | Val Accuracy:  0.000         | Val F1_macro:  0.000         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4gBxOqLW3tL"
      },
      "execution_count": 36,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f707ed687e1cc7dca614d866740125e744cc3f7963ec2d63a60d682146be2e45"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}