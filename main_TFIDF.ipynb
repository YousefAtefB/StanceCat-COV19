{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sh4oK8C6flZ2"
      },
      "outputs": [],
      "source": [
        "# Preprocessing: https://github.com/MagedSaeed/farasapy\n",
        "# Word2vec: https://github.com/bakrianoo/aravec\n",
        "# TF-IDF, PCA?\n",
        "# KNN? KMeans?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MphWnrDxQvzK",
        "outputId": "851e2ea3-ddfd-4951-f072-c3735c613199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Dataset.zip\n",
            "  inflating: Dataset/dev.csv         \n",
            "  inflating: Dataset/train.csv       \n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!unzip Dataset.zip\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfS-kJd2s-VO"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0vddDey3s-VS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchmetrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFLsNzD4s-VT"
      },
      "source": [
        "# Data PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DT2pKLCds-VT"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('Dataset/train.csv')\n",
        "text_train, category_train, stance_train = train['text'], train['category'], train['stance']\n",
        "\n",
        "dev = pd.read_csv('Dataset/dev.csv')\n",
        "text_dev, category_dev, stance_dev = dev['text'], dev['category'], dev['stance']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ILZuDzwXs-VU",
        "outputId": "295d9242-fbb5-4ba7-f18e-4a6fc9d9b501"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text   category  stance\n",
              "0  بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...  celebrity       1\n",
              "1  وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...  info_news       1\n",
              "2  قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...  info_news       1\n",
              "3  #تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...  celebrity       1\n",
              "4  وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...   personal       0\n",
              "5  لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...  info_news       0\n",
              "6  لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...  info_news       1\n",
              "7  من عوارض لقاح كورونا<LF>هو تهكير حسابك عتويتر<...   personal       0\n",
              "8  هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...  unrelated       0\n",
              "9  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...  info_news       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16d2c586-071e-4947-9573-def34d47ee67\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بيل غيتس يتلقى لقاح #كوفيد19 من غير تصوير الاب...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وزير الصحة لحد اليوم وتحديدا هلأ بمؤتمروا الصح...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>قولكن  رح يكونو اد المسؤولية ب لبنان لما يوصل ...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#تركيا.. وزير الصحة فخر الدين قوجة يتلقى أول ج...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>وئام وهاب يشتم الدول الخليجية في كل طلة اعلامي...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>لقاح #كورونا في أميركا.. قلق متزايد من \"التوزي...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>لبنان اشترى مليونان لقاح امريكي اذا شلنا يلي ع...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>من عوارض لقاح كورونا&lt;LF&gt;هو تهكير حسابك عتويتر&lt;...</td>\n",
              "      <td>personal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>هناك 1780 مليونيراً في لبنان. ماذا لو فُرضت ال...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16d2c586-071e-4947-9573-def34d47ee67')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16d2c586-071e-4947-9573-def34d47ee67 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16d2c586-071e-4947-9573-def34d47ee67');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "_31w6mFzs-VU",
        "outputId": "c8bca46e-01be-4b01-840d-09a8abf8db7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text   category  stance\n",
              "0  #مريم_رجوي: <LF>حظر خامنئي المجرم شراء #لقاح_ك...  info_news       1\n",
              "1  #الصحة:<LF>•تم إعطاء 259.530 جرعة من لقاح #كور...       plan       1\n",
              "2  #خادم_الحرمين - حفظه الله - يتلقى الجرعة الأول...  celebrity       1\n",
              "3  #الصحه_العالميه: لقاحات #كورونا آمنة ولا خوف م...  info_news       1\n",
              "4  #وزيرة_الصحة \"#هالة_زايد\" تقول إنه يجرى مراجعة...  info_news       1\n",
              "5  2️⃣ وانتهى الفريق من الدراسات قبل السريرية ونش...  info_news       1\n",
              "6  عاجل 🔴 <LF>.<LF><LF>.<LF><LF>وزارة الصحة :<LF>...       plan       1\n",
              "7  #فيديو | السفير الأميركي لدى السعودية بعد تلقي...  info_news       1\n",
              "8  تصريحات وبس الحكومة مع السيسي علي حسب اللقطة! ...  info_news       0\n",
              "9  الاتحاد الاوروبي تفاوض لشراء لقاحات الكورونا م...  info_news       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96e31574-47f0-497a-95ea-72a62c18adc2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#مريم_رجوي: &lt;LF&gt;حظر خامنئي المجرم شراء #لقاح_ك...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#الصحة:&lt;LF&gt;•تم إعطاء 259.530 جرعة من لقاح #كور...</td>\n",
              "      <td>plan</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#خادم_الحرمين - حفظه الله - يتلقى الجرعة الأول...</td>\n",
              "      <td>celebrity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#الصحه_العالميه: لقاحات #كورونا آمنة ولا خوف م...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#وزيرة_الصحة \"#هالة_زايد\" تقول إنه يجرى مراجعة...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2️⃣ وانتهى الفريق من الدراسات قبل السريرية ونش...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>عاجل 🔴 &lt;LF&gt;.&lt;LF&gt;&lt;LF&gt;.&lt;LF&gt;&lt;LF&gt;وزارة الصحة :&lt;LF&gt;...</td>\n",
              "      <td>plan</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>#فيديو | السفير الأميركي لدى السعودية بعد تلقي...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>تصريحات وبس الحكومة مع السيسي علي حسب اللقطة! ...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>الاتحاد الاوروبي تفاوض لشراء لقاحات الكورونا م...</td>\n",
              "      <td>info_news</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96e31574-47f0-497a-95ea-72a62c18adc2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-96e31574-47f0-497a-95ea-72a62c18adc2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-96e31574-47f0-497a-95ea-72a62c18adc2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dev.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLAvuG6hs-VV",
        "outputId": "a536fe20-4d3f-4e02-c0a2-1d010d31f8d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6988,) (6988,) (6988,)\n",
            "(1000,) (1000,) (1000,)\n"
          ]
        }
      ],
      "source": [
        "text_train, category_train, stance_train = np.array(train['text']), np.array(train['category']), np.array(train['stance'])\n",
        "text_dev, category_dev, stance_dev = np.array(dev['text']), np.array(dev['category']), np.array(dev['stance'])\n",
        "\n",
        "print(text_train.shape, category_train.shape, stance_train.shape)\n",
        "print(text_dev.shape, category_dev.shape, stance_dev.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i_m5xhQz1hC",
        "outputId": "f30125bc-1835-4fad-89d7-78241adbac0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xOLnhIfNs-VV"
      },
      "outputs": [],
      "source": [
        "def PreProcessing(text):\n",
        "\n",
        "    # remove links\n",
        "    text = [re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) for x in text]\n",
        "    # text = [re.sub(r'https?:\\/\\/\\S*', '', x, flags=re.MULTILINE) for x in text]\n",
        "\n",
        "    # remove emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    text = [emoji_pattern.sub(r'', x) for x in text] # no emoji\n",
        "\n",
        "    # remove english words\n",
        "    text = [re.sub(r'\\s*[A-Za-z]+\\b', '' , x) for x in text]\n",
        "\n",
        "    # tokenize\n",
        "    text = [nltk.tokenize.word_tokenize(x) for x in text]\n",
        "\n",
        "    # # remove stop-words\n",
        "    # stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
        "\n",
        "    # for i in range(len(text)):\n",
        "    #     text[i] = [word for word in text[i] if word not in stopwords]\n",
        "\n",
        "\n",
        "    for i in range(len(text)):\n",
        "        text[i] = [word for word in text[i] if len(word)>2]\n",
        "\n",
        "    # but anything in empty strings\n",
        "    for i in range(len(text)):\n",
        "        if(len(text[i])==0):\n",
        "            text[i]='<unk>'\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKSvPyvPs-VW",
        "outputId": "a7dd02d5-6c97-4879-b33c-3d080780aed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الامريكيين متهمون بصنع ونشر فيروس كورونا ولذلك لا يمكن الوثوق بهم”<LF>الإمام الخامنئي<LF><LF>#لقاح_آمن\tinfo_news\t0\n",
            "train\tحبيبنا وقرة أعيننا سيدي #خادم_الحرمين_الشريفين الملك سلمان حفظه الله يتلقى الجرعة الأولى من لقاح كورنا … نفعه الله به ومتعه بالصحة والعافيه. https://t.co/AJRzC7dCWe\tcelebrity\t1\n",
            "train\tرغم تلقيه جرعتين من لقاح #فايزر.. إصابة كبير حاخامات #تل_أبيب، يسرائيل لاو،  83 عاما، بفيروس #كورونا، حيث انتقلت له العدوى من زوجته بعد مخالطتها مصابا آخر https://t.co/RGI6WTgrxf\tcelebrity\t0\n",
            "train\tتلقيت قبل قليل الجرعة الثانية من لقاح كورونا، وكلي فخر بجهود وطننا الغالي وتوجيهات قيادتنا الرشيدة التي تؤكد أن صحة الإنسان أولاً.  🇸🇦🇸🇦🇸🇦🇸🇦 https://t.co/XGstr9Zvzf\tinfo_news\t1\n",
            "train\tشركة صحة\": جزيل الشكر للمواطنة ملهية شويرب سعيد العامري، التي تبلغ ١٠٢ عاماً<LF>لكونها قدوة لجميع أفراد المجتمع من خلال <LF> تلقيها أول جرعة من لقاح كوفيد-19 في مركز القوع الصحي #الإمارات_اليوم https://t.co/uBSCd0JZ4Y\n"
          ]
        }
      ],
      "source": [
        "print(max(text_train, key=len))\n",
        "text_train = PreProcessing(text_train)\n",
        "text_dev = PreProcessing(text_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrlO0qz8s-VW",
        "outputId": "b6662047-0dad-4087-b4fe-c1a9c4cea0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['بيل', 'غيتس', 'يتلقى', 'لقاح', 'كوفيد19', 'غير', 'تصوير', 'الابرة', 'السيرنجة', 'الدواء', 'لابس', 'بولو', 'صيفي', 'الشتاء', 'يقول', 'إحدى', 'مزايا', 'عمر', 'عامًا', 'انه', 'مؤهل', 'للحصول', 'على', 'اللقاح', '...', 'يعنى', 'كان', 'يحتاج', 'اللقاح', 'كان', 'عمره', 'اصغر']\n"
          ]
        }
      ],
      "source": [
        "with open('processed_train.txt','w', encoding='utf8') as f:\n",
        "\tfor i in text_train:\n",
        "\t\tf.write('%s\\n'%i)\n",
        "print(text_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFIDF"
      ],
      "metadata": {
        "id": "UF4BHmaBj9x_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f7KTsZq7r24b"
      },
      "outputs": [],
      "source": [
        "def get_TFIDF(in_corpus): # list of list of str\n",
        "  corpus = [' '.join(l) for l in in_corpus]\n",
        "  print('corpus size: ', len(corpus))\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X = vectorizer.fit_transform(corpus)\n",
        "  return X.getH().toarray(), X.toarray(), vectorizer.get_feature_names_out() # list of words, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zbL8uawMtAxT"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(word1, word2, vocab):\n",
        "  # vocab = vectorizer_TF_IDF.get_feature_names_out()\n",
        "  ind1 = np.where(vocab==word1)[0].item()\n",
        "  ind2 = np.where(vocab==word2)[0].item()\n",
        "  return np.dot(temp_TF_IDF[ind1], temp_TF_IDF[ind2])/(np.sqrt(np.sum(temp_TF_IDF[ind1]**2))*np.sqrt(np.sum(temp_TF_IDF[ind2]**2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OuNUZ9YWvNOK"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "def apply_PCA(list_of_features, n_components): # take best n_components features for all samples\n",
        "  pca = PCA(n_components=n_components)\n",
        "  return pca.fit_transform(list_of_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rn-k8mD00Lbi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_annotated(points, labels=[]):\n",
        "  \"\"\"\n",
        "  [[0 1]\n",
        "  [2 3]]\n",
        "  -> (0,1), (2, 3)\n",
        "  \"\"\"\n",
        "  plt.scatter(points[:,0], points[:,1])\n",
        "  for i, txt in enumerate(labels):\n",
        "    plt.annotate(txt, (points[i,0], points[i,1]))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "lG5MvaMLgJM9",
        "outputId": "7235bcd2-9251-41b2-d92b-a7608afd994d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus size:  4\n",
            "9 9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.813894995377407\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "corpus_TF_IDF = [\n",
        "    ['This','is', 'the', 'first', 'document.'],\n",
        "    ['This', 'document', 'is', 'the', 'second', 'document.'],\n",
        "    ['And', 'this', 'is', 'the', 'third', 'one.'],\n",
        "    ['Is', 'this', 'the', 'first', 'document?'],\n",
        "]\n",
        "temp_TF_IDF, temp2_TF_IDF, vocab_tf_idf = get_TFIDF(corpus_TF_IDF)\n",
        "after_pca = apply_PCA(temp_TF_IDF, 2)\n",
        "print(len(temp_TF_IDF), len(after_pca))\n",
        "plot_annotated(after_pca, vocab_tf_idf)\n",
        "print(cosine_similarity('this', 'first', vocab_tf_idf))\n",
        "print(cosine_similarity('this', 'is', vocab_tf_idf))\n",
        "print(cosine_similarity('one', 'third', vocab_tf_idf))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_dim = 100"
      ],
      "metadata": {
        "id": "r80f2zWKK9Tq"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Cc6Oq9Rj8v",
        "outputId": "03199bcd-070b-4f08-c3d2-3b5461c28c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus size:  6988\n",
            "corpus size:  1000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "reduced_tfidf_train = apply_PCA(get_TFIDF(text_train)[1], word_dim)\n",
        "reduced_tfidf_dev = apply_PCA(get_TFIDF(text_dev)[1], word_dim)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FFHQRb_W9G3",
        "outputId": "23a3a2a5-5975-4a06-c53a-4357f45d6ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6988\n",
            "6988\n"
          ]
        }
      ],
      "source": [
        "print(len(reduced_tfidf_train))\n",
        "print(len(text_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u81ewM-6R_R-",
        "outputId": "578357e2-fa4b-4aba-f8bd-5a7f8f6afa84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "698800\n"
          ]
        }
      ],
      "source": [
        "print(len(reduced_tfidf_train[0]))\n",
        "print(reduced_tfidf_train.size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stance_dev.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2lj9nh9NKR6",
        "outputId": "03d55f81-83e9-40bd-d0f0-590f514d8a8d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NB_TFIDF_stance():\n",
        "  from sklearn.naive_bayes import GaussianNB\n",
        "  from sklearn.metrics import f1_score\n",
        "  gnb = GaussianNB()\n",
        "  y_pred_tfidf = gnb.fit(reduced_tfidf_train, stance_train).predict(reduced_tfidf_dev)\n",
        "  print(\"tfidf NB stance accuracy = \", np.sum(y_pred_tfidf==stance_dev)/stance_dev.size)\n",
        "  f1Score_tfidf = f1_score(y_pred_tfidf, stance_dev, average='macro')\n",
        "  print(\"tfidf NB stance f1 score = \", f1Score_tfidf)\n",
        "\n",
        "NB_TFIDF_stance()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhrUzpUkKTdk",
        "outputId": "e22c73bf-30ac-45b5-bb59-1e798b1ac8db"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf NB stance accuracy =  0.716\n",
            "tfidf NB stance f1 score =  0.42111751821081295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NB_TFIDF_category():\n",
        "  from sklearn.naive_bayes import GaussianNB\n",
        "  from sklearn.metrics import f1_score\n",
        "  gnb = GaussianNB()\n",
        "  y_pred_tfidf = gnb.fit(reduced_tfidf_train, category_train).predict(reduced_tfidf_dev)\n",
        "  print(\"tfidf NB categories accuracy = \", np.sum(y_pred_tfidf==category_dev)/category_dev.size)\n",
        "  f1Score_tfidf = f1_score(y_pred_tfidf, category_dev, average='macro')\n",
        "  print(\"tfidf NB categories f1 score = \", f1Score_tfidf)\n",
        "\n",
        "NB_TFIDF_category()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jipneod5apZo",
        "outputId": "e620cb27-2b72-4cd1-be95-df5c56939417"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf NB categories accuracy =  0.392\n",
            "tfidf NB categories f1 score =  0.1679505170536444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SVM_TFIDF_stance():\n",
        "  from sklearn import svm\n",
        "  from sklearn.metrics import f1_score\n",
        "  clf = svm.SVC()\n",
        "  clf.fit(reduced_tfidf_train, stance_train)\n",
        "  y_pred_tfidf = clf.predict(reduced_tfidf_dev)\n",
        "  print(\"tfidf SVM stance accuracy = \", np.sum(y_pred_tfidf==stance_dev)/stance_dev.size)\n",
        "  f1Score_tfidf = f1_score(y_pred_tfidf, stance_dev, average='macro')\n",
        "  print(\"tfidf SVM stance f1 score = \", f1Score_tfidf)\n",
        "\n",
        "SVM_TFIDF_stance()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiW0-Vs4RUpp",
        "outputId": "a213b05b-d8db-44d0-e459-955bbe7ec85c"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf SVM stance accuracy =  0.804\n",
            "tfidf SVM stance f1 score =  0.29711751662971175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SVM_TFIDF_category():\n",
        "  from sklearn import svm\n",
        "  from sklearn.metrics import f1_score\n",
        "  clf = svm.SVC()\n",
        "  clf.fit(reduced_tfidf_train, category_train)\n",
        "  y_pred_tfidf = clf.predict(reduced_tfidf_dev)\n",
        "  print(\"tfidf SVM categories accuracy = \", np.sum(y_pred_tfidf==category_dev)/category_dev.size)\n",
        "  f1Score_tfidf = f1_score(y_pred_tfidf, category_dev, average='macro')\n",
        "  print(\"tfidf SVM categories f1 score = \", f1Score_tfidf)\n",
        "\n",
        "SVM_TFIDF_category()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4TFvU0Ja3iY",
        "outputId": "ccc59cad-b78d-4364-8b3b-54225813767c"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf SVM categories accuracy =  0.586\n",
            "tfidf SVM categories f1 score =  0.12828328128091956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "LoL5PI9Rj6LX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "mIcGm0Nns-VX"
      },
      "outputs": [],
      "source": [
        "def BuildVocab(text, pad='<pad>', unk='<unk>'):\n",
        "\n",
        "    vocab = set()    \n",
        "    for x in text:\n",
        "        vocab |= set(x)\n",
        "\n",
        "    vocab = [pad, unk] + list(vocab)\n",
        "\n",
        "    id2word = {i: word for i, word in enumerate(vocab)}\n",
        "    word2id = {word: i for i, word in id2word.items()}\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    return vocab_size, vocab, id2word, word2id    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nftlQZoTs-VX",
        "outputId": "731a82f2-7300-4de7-f4f7-7808bf067594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32058\n"
          ]
        }
      ],
      "source": [
        "vocab_size, vocab, id2word, word2id = BuildVocab(text_train)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZaQFz0ys-VX",
        "outputId": "062e1998-c593-434b-c888-3de980d78e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'plan', 'restrictions', 'unrelated', 'info_news', 'others', 'personal', 'rumors', 'advice', 'requests', 'celebrity'}\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "categories = set(category_train)\n",
        "print(categories)\n",
        "category2id = {word:i for i, word in enumerate(list(categories))}\n",
        "print(category2id['celebrity'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87f0flhJs-VY"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sL323A7s-VY"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSoy8cSps-VY"
      },
      "source": [
        "### Ideas to try\n",
        "1) bi-directional\n",
        "2) pre-training\n",
        "3) multi-layers\n",
        "4) BERT\n",
        "5) transformers notebook\n",
        "6) packed_padded_sequences\n",
        "7) pre-trained embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vQ8xsOes-VY"
      },
      "source": [
        "### Building Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "K5vedTo_s-VY"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, x, y, pad='<pad>', unk='<unk>', word2id=word2id):\n",
        "\n",
        "    x = x.copy()\n",
        "\n",
        "    # src lengths to be used in pack padded\n",
        "    self.seq_lengths = torch.LongTensor(list(map(len, x)))\n",
        "\n",
        "    print(x[0], self.seq_lengths[0])\n",
        "\n",
        "    for i in range(len(x)):\n",
        "      x[i] = [word2id[word] if word in word2id else word2id[unk] for word in x[i]]\n",
        "    \n",
        "    print(x[0])\n",
        "\n",
        "    self.X = torch.nn.utils.rnn.pad_sequence([torch.tensor(sentence) for sentence in x], batch_first=True, padding_value=word2id[pad])\n",
        "\n",
        "    # sort sequeces decreasing in size\n",
        "    self.seq_lengths, perm_idx = self.seq_lengths.sort(0, descending=True)\n",
        "    self.X = self.X[perm_idx]\n",
        "\n",
        "    print(self.X[0])\n",
        "\n",
        "    print(self.X.shape)\n",
        "\n",
        "    print(min(self.seq_lengths))\n",
        "    \n",
        "    self.Y = torch.tensor(y)\n",
        "    self.len = len(x)\n",
        "    self.pad = pad\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.Y[idx], self.seq_lengths[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFZQMUCes-VY",
        "outputId": "bc9002d1-fffa-4ef1-f6e0-f195a13cbb69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['بيل', 'غيتس', 'يتلقى', 'لقاح', 'كوفيد19', 'غير', 'تصوير', 'الابرة', 'السيرنجة', 'الدواء', 'لابس', 'بولو', 'صيفي', 'الشتاء', 'يقول', 'إحدى', 'مزايا', 'عمر', 'عامًا', 'انه', 'مؤهل', 'للحصول', 'على', 'اللقاح', '...', 'يعنى', 'كان', 'يحتاج', 'اللقاح', 'كان', 'عمره', 'اصغر'] tensor(32)\n",
            "[8, 19696, 7505, 6768, 26894, 15372, 10657, 12699, 18035, 11124, 31879, 21228, 24528, 27004, 5517, 25780, 19912, 6904, 18880, 27368, 26892, 16581, 18459, 5269, 21805, 19643, 12605, 17578, 5269, 12605, 3001, 3829]\n",
            "tensor([13360, 23736,  2452, 16041,  1731,  3149,  7740,  6573, 25038, 10802,\n",
            "        10549, 13839, 10339, 12268, 10742, 19217, 18778, 18598, 27933, 10320,\n",
            "        21477, 21467, 22111,  7505, 16068, 30955,  6768, 18079, 27205, 22111,\n",
            "        12894,  2717, 16242,  2124,  8301, 12000,  6768,  4332, 31647, 31874,\n",
            "        23156, 32041,  3675,  7362, 19248, 25325, 17472,    81,  9900, 29497,\n",
            "        18001, 19120, 29957, 31385, 12620, 23031, 20704, 21376, 16068, 16737,\n",
            "         6768, 17472, 31209, 20908, 29958,  7343, 27734,  1450, 26847,  6666,\n",
            "        13286,  1412, 30148, 27350, 28984, 27966, 30148, 30060,  6975, 30019,\n",
            "        26957, 23632, 23457, 13454, 13286,  1068, 24147, 23646,  9735, 28089,\n",
            "        18551, 22645, 14258, 16393,  8733, 18718,  1629,  6768,  8534, 12222,\n",
            "        18777, 31638,  7567])\n",
            "torch.Size([6988, 103])\n",
            "tensor(1)\n",
            "['بيل', 'غيتس', 'يتلقى', 'لقاح', 'كوفيد19', 'غير', 'تصوير', 'الابرة', 'السيرنجة', 'الدواء', 'لابس', 'بولو', 'صيفي', 'الشتاء', 'يقول', 'إحدى', 'مزايا', 'عمر', 'عامًا', 'انه', 'مؤهل', 'للحصول', 'على', 'اللقاح', '...', 'يعنى', 'كان', 'يحتاج', 'اللقاح', 'كان', 'عمره', 'اصغر'] tensor(32)\n",
            "[8, 19696, 7505, 6768, 26894, 15372, 10657, 12699, 18035, 11124, 31879, 21228, 24528, 27004, 5517, 25780, 19912, 6904, 18880, 27368, 26892, 16581, 18459, 5269, 21805, 19643, 12605, 17578, 5269, 12605, 3001, 3829]\n",
            "tensor([13360, 23736,  2452, 16041,  1731,  3149,  7740,  6573, 25038, 10802,\n",
            "        10549, 13839, 10339, 12268, 10742, 19217, 18778, 18598, 27933, 10320,\n",
            "        21477, 21467, 22111,  7505, 16068, 30955,  6768, 18079, 27205, 22111,\n",
            "        12894,  2717, 16242,  2124,  8301, 12000,  6768,  4332, 31647, 31874,\n",
            "        23156, 32041,  3675,  7362, 19248, 25325, 17472,    81,  9900, 29497,\n",
            "        18001, 19120, 29957, 31385, 12620, 23031, 20704, 21376, 16068, 16737,\n",
            "         6768, 17472, 31209, 20908, 29958,  7343, 27734,  1450, 26847,  6666,\n",
            "        13286,  1412, 30148, 27350, 28984, 27966, 30148, 30060,  6975, 30019,\n",
            "        26957, 23632, 23457, 13454, 13286,  1068, 24147, 23646,  9735, 28089,\n",
            "        18551, 22645, 14258, 16393,  8733, 18718,  1629,  6768,  8534, 12222,\n",
            "        18777, 31638,  7567])\n",
            "torch.Size([6988, 103])\n",
            "tensor(1)\n",
            "['مريم_رجوي', 'حظر', 'خامنئي', 'المجرم', 'شراء', 'لقاح_كورونا', 'يعد', 'مجزرة', 'متعمدة', 'بحق', 'الشعب', 'الإيراني', 'نقل', 'موقع', 'مريم', 'رجوي', 'موقف', 'رئيسة', 'الجمهورية', 'المنتخبة', 'للمقاومة', 'الإيرانية', 'تصريحات', 'خامنئي', 'المجرم', 'حول', 'حظر', 'استيراد', 'لقاح', 'كورونا', 'الولايات', 'المتحدة', 'بريطانيا', 'فرنسا', 'اللقاح_حق_للناس'] tensor(35)\n",
            "[18249, 26496, 11975, 28533, 11010, 7288, 16291, 3174, 32043, 8634, 22138, 16312, 28716, 3614, 26392, 24912, 10403, 14492, 18793, 1, 1, 24246, 16764, 11975, 28533, 10063, 26496, 13949, 6768, 3149, 5405, 18951, 8267, 30797, 18050]\n",
            "tensor([  997,     1,     1,     1, 26946, 27853, 11044, 31814,  7036,     1,\n",
            "        10811, 13771, 17137,     1,  2783,     1,     1, 17307,     1, 18352,\n",
            "         9560, 25198, 19120, 25198,   997,     1,     1,     1,     1,     1,\n",
            "        12639,  7201, 29158, 22111, 18621,     1,  3149, 29607,     1,  5126,\n",
            "            1, 14905, 21205,     1,  3164])\n",
            "torch.Size([1000, 45])\n",
            "tensor(2)\n",
            "['مريم_رجوي', 'حظر', 'خامنئي', 'المجرم', 'شراء', 'لقاح_كورونا', 'يعد', 'مجزرة', 'متعمدة', 'بحق', 'الشعب', 'الإيراني', 'نقل', 'موقع', 'مريم', 'رجوي', 'موقف', 'رئيسة', 'الجمهورية', 'المنتخبة', 'للمقاومة', 'الإيرانية', 'تصريحات', 'خامنئي', 'المجرم', 'حول', 'حظر', 'استيراد', 'لقاح', 'كورونا', 'الولايات', 'المتحدة', 'بريطانيا', 'فرنسا', 'اللقاح_حق_للناس'] tensor(35)\n",
            "[18249, 26496, 11975, 28533, 11010, 7288, 16291, 3174, 32043, 8634, 22138, 16312, 28716, 3614, 26392, 24912, 10403, 14492, 18793, 1, 1, 24246, 16764, 11975, 28533, 10063, 26496, 13949, 6768, 3149, 5405, 18951, 8267, 30797, 18050]\n",
            "tensor([  997,     1,     1,     1, 26946, 27853, 11044, 31814,  7036,     1,\n",
            "        10811, 13771, 17137,     1,  2783,     1,     1, 17307,     1, 18352,\n",
            "         9560, 25198, 19120, 25198,   997,     1,     1,     1,     1,     1,\n",
            "        12639,  7201, 29158, 22111, 18621,     1,  3149, 29607,     1,  5126,\n",
            "            1, 14905, 21205,     1,  3164])\n",
            "torch.Size([1000, 45])\n",
            "tensor(2)\n"
          ]
        }
      ],
      "source": [
        "stance_train_dataset = Dataset(text_train, stance_train + 1)\n",
        "category_train_dataset = Dataset(text_train, [category2id[category] for category in category_train])\n",
        "\n",
        "stance_dev_dataset = Dataset(text_dev, stance_dev + 1)\n",
        "category_dev_dataset = Dataset(text_dev, [category2id[category] for category in category_dev])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "c8cEyA6Cs-VZ"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, dropout=dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(src)\n",
        "        embedded = self.dropout(embedded)\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        packed_embedded =  torch.nn.utils.rnn.pack_padded_sequence(embedded, src_len.cpu().numpy(), batch_first=False)\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        #outputs = [src len, batch size, hid dim]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        #outputs are always from the top hidden layer\n",
        "\n",
        "        prediction = self.fc_out(hidden)\n",
        "        #prediction = [1, batch size, output dim]\n",
        "\n",
        "        prediction = prediction.squeeze(0)\n",
        "        #prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "c5GVwWITs-VZ"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIN9pl-Os-VZ"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "E5FS34QHs-VZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, train_dataset, batch_size=512, epochs=10, learning_rate=0.01):\n",
        "  \n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "  # criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  \n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  for epoch_num in range(epochs):\n",
        "    total_acc_train = 0.0\n",
        "    total_loss_train = 0.0\n",
        "    all_output_labels = None\n",
        "    all_desired_labels = None\n",
        "    for train_input, train_label, src_len in tqdm(train_dataloader):\n",
        "\n",
        "      train_input = train_input.to(device).permute(1, 0)\n",
        "      # print(train_input.shape)\n",
        "      train_label = train_label.to(device)\n",
        "\n",
        "      output = model(train_input, src_len)\n",
        "\n",
        "      # print(output.shape, train_label.shape)\n",
        "      \n",
        "      batch_loss = criterion(output.view(-1, model.output_dim), train_label.view(-1))\n",
        "\n",
        "      total_loss_train += batch_loss\n",
        "      \n",
        "      predicted_labels = torch.argmax(output, -1)\n",
        "      acc = torch.sum(predicted_labels == train_label) \n",
        "      total_acc_train += acc\n",
        "\n",
        "\n",
        "      all_output_labels = torch.cat((all_output_labels, predicted_labels)) if all_output_labels is not None else predicted_labels\n",
        "      all_desired_labels = torch.cat((all_desired_labels, train_label)) if all_desired_labels is not None else train_label\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      \n",
        "    epoch_loss = total_loss_train / len(train_dataset)\n",
        "\n",
        "    epoch_acc = total_acc_train / len(train_dataset)\n",
        "\n",
        "    f1Score_fun = torchmetrics.F1Score(task=\"multiclass\", num_classes=model.output_dim, average='macro')\n",
        "\n",
        "    print(all_output_labels)\n",
        "    print(all_desired_labels)\n",
        "    f1Score = f1Score_fun(all_output_labels, all_desired_labels)\n",
        "\n",
        "    print(\n",
        "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
        "        | Train Accuracy: {epoch_acc} | F1 Score: {f1Score}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "js2LuPgZs-VZ"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = vocab_size\n",
        "OUTPUT_DIM = 3\n",
        "EMB_DIM = 50 #256\n",
        "HID_DIM = 50 #512\n",
        "DROPOUT = 0.0\n",
        "\n",
        "stance_model = LSTM(INPUT_DIM, EMB_DIM, HID_DIM, OUTPUT_DIM, DROPOUT).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcENdYCVU-DW",
        "outputId": "fab73a41-91d6-47a8-b60d-78f76ff636fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(32058, 50)\n"
          ]
        }
      ],
      "source": [
        "print(stance_model.embedding.float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "6VBlHviGStbl"
      },
      "outputs": [],
      "source": [
        "# comparison_list = []\n",
        "# l1 = stance_model.embedding(stance_train_dataset.X).detach().numpy()\n",
        "# print(len(l1))\n",
        "# print(len(reduced_tfidf_embeddings))\n",
        "# for i in range(len(reduced_tfidf_embeddings)):\n",
        "#   temp = np.dot(l1[i], reduced_tfidf_embeddings[i]) / (np.sqrt(np.sum(reduced_tfidf_embeddings[i]**2))*np.sqrt(np.sum(l1[i]**2)))\n",
        "#   comparison_list.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp094MPOs-Va",
        "outputId": "0c6fa480-941f-4216-c184-a3ede3a57fa9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(32058, 50)\n",
              "  (lstm): LSTM(50, 50)\n",
              "  (fc_out): Linear(in_features=50, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "stance_model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5WXGxPTs-Va",
        "outputId": "ba3d82e0-41eb-492c-dd68-f778b1da4e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,623,453 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(stance_model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMf3d_TZs-Va",
        "outputId": "f02c3ea2-09f6-4026-80ec-5210c7bb4bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 1 | Train Loss: 0.0015010422794148326         | Train Accuracy: 0.7925014495849609 | F1 Score: 0.29474693536758423\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 2 | Train Loss: 0.0012273920001462102         | Train Accuracy: 0.7925014495849609 | F1 Score: 0.29474693536758423\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 3 | Train Loss: 0.0009096839930862188         | Train Accuracy: 0.8106754422187805 | F1 Score: 0.42098042368888855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 1, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 4 | Train Loss: 0.0006230577710084617         | Train Accuracy: 0.883085310459137 | F1 Score: 0.5466014742851257\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 1, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 5 | Train Loss: 0.0005562735605053604         | Train Accuracy: 0.8938179612159729 | F1 Score: 0.56060791015625\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 1, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 6 | Train Loss: 0.0005049277096986771         | Train Accuracy: 0.9068403244018555 | F1 Score: 0.6822534203529358\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 1, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 7 | Train Loss: 0.00037350456113927066         | Train Accuracy: 0.9368917942047119 | F1 Score: 0.8410977125167847\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 8 | Train Loss: 0.0004146290011703968         | Train Accuracy: 0.9274470806121826 | F1 Score: 0.8426430225372314\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 9 | Train Loss: 0.00041122001130133867         | Train Accuracy: 0.9367486834526062 | F1 Score: 0.8625155687332153\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 1, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 10 | Train Loss: 0.0004034863377455622         | Train Accuracy: 0.9301660060882568 | F1 Score: 0.8457803130149841\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 11 | Train Loss: 0.0002502267016097903         | Train Accuracy: 0.9583571553230286 | F1 Score: 0.9060056209564209\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 12 | Train Loss: 0.000201490824110806         | Train Accuracy: 0.9642243981361389 | F1 Score: 0.9217232465744019\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 13 | Train Loss: 0.00018927959899883717         | Train Accuracy: 0.9670864343643188 | F1 Score: 0.9302529096603394\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 14 | Train Loss: 0.00016893511929083616         | Train Accuracy: 0.9690898656845093 | F1 Score: 0.9347975254058838\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 15 | Train Loss: 0.00014989846386015415         | Train Accuracy: 0.9725243449211121 | F1 Score: 0.9428045153617859\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 16 | Train Loss: 0.0001368038065265864         | Train Accuracy: 0.9735260605812073 | F1 Score: 0.9437968730926514\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 17 | Train Loss: 0.00012787191371899098         | Train Accuracy: 0.9743846654891968 | F1 Score: 0.9467681646347046\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 18 | Train Loss: 0.00012020522990496829         | Train Accuracy: 0.9752432703971863 | F1 Score: 0.9481027126312256\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 19 | Train Loss: 0.00011339654884068295         | Train Accuracy: 0.9769604802131653 | F1 Score: 0.9521799087524414\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Epochs: 20 | Train Loss: 0.0001131380267906934         | Train Accuracy: 0.9758157134056091 | F1 Score: 0.9508205652236938\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train(stance_model, stance_train_dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "VnZ14eygs-Va"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = vocab_size\n",
        "OUTPUT_DIM = 10\n",
        "EMB_DIM = 50 #256\n",
        "HID_DIM = 50 #512\n",
        "DROPOUT = 0.0\n",
        "\n",
        "category_model = LSTM(INPUT_DIM, EMB_DIM, HID_DIM, OUTPUT_DIM, DROPOUT).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeC3kutMs-Va",
        "outputId": "9c36eddc-d943-481f-ed68-d8ae4517113d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(32058, 50)\n",
              "  (lstm): LSTM(50, 50)\n",
              "  (fc_out): Linear(in_features=50, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "category_model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5XlLjNOs-Vb",
        "outputId": "ea046fb4-2c35-415f-c7eb-e59af6a10215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,623,810 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(category_model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUarEQIWs-Vb",
        "outputId": "b52ddc03-2917-475d-9e0c-8db05e6de85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 5, 5,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 1 | Train Loss: 0.003550139255821705         | Train Accuracy: 0.43889525532722473 | F1 Score: 0.08081360161304474\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 2 | Train Loss: 0.0030367986764758825         | Train Accuracy: 0.5174584984779358 | F1 Score: 0.06820067763328552\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 3 | Train Loss: 0.0028640441596508026         | Train Accuracy: 0.5186033248901367 | F1 Score: 0.07057829946279526\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 4 | Train Loss: 0.0024570282548666         | Train Accuracy: 0.596737265586853 | F1 Score: 0.1544865518808365\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 5, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 5 | Train Loss: 0.001949188532307744         | Train Accuracy: 0.6924728155136108 | F1 Score: 0.253777414560318\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 9, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 6 | Train Loss: 0.0016158450162038207         | Train Accuracy: 0.7442759275436401 | F1 Score: 0.308613121509552\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 5, 3,  ..., 3, 3, 9])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 7 | Train Loss: 0.0013089862186461687         | Train Accuracy: 0.7997996807098389 | F1 Score: 0.3876025676727295\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 8 | Train Loss: 0.0012716333149001002         | Train Accuracy: 0.8043789267539978 | F1 Score: 0.4051602780818939\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 0, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 9 | Train Loss: 0.001260849996469915         | Train Accuracy: 0.8008013963699341 | F1 Score: 0.39060020446777344\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 5, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 10 | Train Loss: 0.0011377468472346663         | Train Accuracy: 0.829994261264801 | F1 Score: 0.43045973777770996\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 11 | Train Loss: 0.0010965637629851699         | Train Accuracy: 0.839009702205658 | F1 Score: 0.466233491897583\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 12 | Train Loss: 0.0009533627890050411         | Train Accuracy: 0.8501717448234558 | F1 Score: 0.4842141270637512\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 13 | Train Loss: 0.0008326356182806194         | Train Accuracy: 0.8709215521812439 | F1 Score: 0.5190879106521606\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:04<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 14 | Train Loss: 0.0007231909548863769         | Train Accuracy: 0.8936748504638672 | F1 Score: 0.6291194558143616\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 15 | Train Loss: 0.0005737235187552869         | Train Accuracy: 0.9114195704460144 | F1 Score: 0.7016768455505371\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 16 | Train Loss: 0.0005066758021712303         | Train Accuracy: 0.9221522808074951 | F1 Score: 0.7478197813034058\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 17 | Train Loss: 0.00045681154006160796         | Train Accuracy: 0.929593563079834 | F1 Score: 0.7924187183380127\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 2, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 18 | Train Loss: 0.0004134778573643416         | Train Accuracy: 0.9367486834526062 | F1 Score: 0.8495901226997375\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 19 | Train Loss: 0.00039205237408168614         | Train Accuracy: 0.9361763000488281 | F1 Score: 0.8577417135238647\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:03<00:00,  3.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "tensor([9, 3, 3,  ..., 3, 3, 3])\n",
            "Epochs: 20 | Train Loss: 0.00042489016777835786         | Train Accuracy: 0.9337435364723206 | F1 Score: 0.8714468479156494\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train(category_model, category_train_dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBBpcizcs-Vb"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "0jFzDy0us-Vb"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dataset, batch_size=512):\n",
        "\n",
        "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  model = model.to(device)\n",
        "\n",
        "  total_acc_test = 0.0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    all_output_labels = None\n",
        "    all_desired_labels = None\n",
        "    for test_input, test_label, src_len in tqdm(test_dataloader):\n",
        "\n",
        "      test_input = test_input.to(device).permute(1, 0)\n",
        "      test_label = test_label.to(device)\n",
        "\n",
        "\n",
        "      output = model(test_input, src_len)\n",
        "\n",
        "      acc = torch.sum(torch.argmax(output, -1)==test_label)\n",
        "      total_acc_test += acc\n",
        "      \n",
        "      all_output_labels = torch.cat((all_output_labels, torch.argmax(output, -1))) if all_output_labels is not None else torch.argmax(output, -1)\n",
        "      all_desired_labels = torch.cat((all_desired_labels, test_label)) if all_desired_labels is not None else test_label\n",
        "\n",
        "\n",
        "    total_acc_test /= len(test_dataset)\n",
        "    f1Score = torchmetrics.F1Score(task=\"multiclass\", num_classes=model.output_dim, average='macro')\n",
        "      \n",
        "    f1Score = f1Score(all_output_labels, all_desired_labels)  \n",
        "  print(f'\\nDev Accuracy: {total_acc_test} | f1Score: {f1Score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhg9352ts-Vb",
        "outputId": "6de53d86-8b4b-4e12-d009-c4f3453aafae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 30.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dev Accuracy: 0.7269999980926514 | f1Score: 0.3150275945663452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(stance_model, stance_dev_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5cL97QXs-Vb",
        "outputId": "e05c6d4a-b837-4f90-caa6-099084faf3a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 27.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dev Accuracy: 0.2930000126361847 | f1Score: 0.0964256003499031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(category_model, category_dev_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "sj1tw3H0s-Vc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "f707ed687e1cc7dca614d866740125e744cc3f7963ec2d63a60d682146be2e45"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}